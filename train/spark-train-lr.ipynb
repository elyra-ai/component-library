{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Logistic Regression classifier with Apache SparkML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.1.2\n",
      "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
      "Requirement already satisfied: py4j==0.10.9 in /opt/conda/lib/python3.8/site-packages (from pyspark==3.1.2) (0.10.9)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=a7f0b4ff948470cc010a81cbad8c6424a17cecd32f233486059d500d39029167\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/df/88/9e/58ef1f74892fef590330ca0830b5b6d995ba29b44f977b3926\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.1.1\n",
      "    Uninstalling pyspark-3.1.1:\n",
      "      Successfully uninstalled pyspark-3.1.1\n",
      "Successfully installed pyspark-3.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget==3.2 in /opt/conda/lib/python3.8/site-packages (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark2pmml==0.5.1\n",
      "  Downloading pyspark2pmml-0.5.1.tar.gz (1.6 kB)\n",
      "Requirement already satisfied: py4j in /opt/conda/lib/python3.8/site-packages (from pyspark2pmml==0.5.1) (0.10.9)\n",
      "Building wheels for collected packages: pyspark2pmml\n",
      "  Building wheel for pyspark2pmml (setup.py): started\n",
      "  Building wheel for pyspark2pmml (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark2pmml: filename=pyspark2pmml-0.5.1-py3-none-any.whl size=2418 sha256=b0c29f9710f3e0809f823b3271b3d9bbab8ab0ce4a5655393a3842dfc8c5ec50\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/fd/9f/02/ce0d5f8237f50df7c58196d4e88f7fb59f6d077372de102fb1\n",
      "Successfully built pyspark2pmml\n",
      "Installing collected packages: pyspark2pmml\n",
      "Successfully installed pyspark2pmml-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "\n",
    "\n",
    "def install(package):\n",
    "    if hasattr(pip, 'main'):\n",
    "        pip.main(['install', package])\n",
    "    else:\n",
    "        pip._internal.main(['install', package])\n",
    "\n",
    "\n",
    "install('pyspark==3.1.2')\n",
    "install('wget==3.2')\n",
    "install('pyspark2pmml==0.5.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "import os\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import wget\n",
    "import sys\n",
    "import shutil\n",
    "import site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Currently only python 3.6 and 3.8 is supported, in case you need a different version please open an issue at https://github.com/elyra-ai/component-library/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a9c7f0d71c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     raise Exception('Currently only python 3.6 and 3.8 is supported, in case '\n\u001b[0m\u001b[1;32m     13\u001b[0m                     \u001b[0;34m+\u001b[0m \u001b[0;34m'you need a different version please open an issue at '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     + 'https://github.com/elyra-ai/component-library/issues')\n",
      "\u001b[0;31mException\u001b[0m: Currently only python 3.6 and 3.8 is supported, in case you need a different version please open an issue at https://github.com/elyra-ai/component-library/issues"
     ]
    }
   ],
   "source": [
    "if sys.version[0:3] == '3.8':\n",
    "    url = 'https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "    + 'jpmml-sparkml-executable-1.7.2.jar'\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar', site.getsitepackages()[0]\n",
    "                + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.6':\n",
    "    url = 'https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "    + 'jpmml-sparkml-executable-1.5.12.jar'\n",
    "    wget.download(url)\n",
    "else:\n",
    "    raise Exception('Currently only python 3.6 and 3.8 is supported, in case '\n",
    "                    + 'you need a different version please open an issue at '\n",
    "                    + 'https://github.com/elyra-ai/component-library/issues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = os.environ.get('data_parquet',\n",
    "                              'data.parquet')  # input file name (parquet)\n",
    "master = os.environ.get('master',\n",
    "                        \"local[*]\")  # URL to Spark master\n",
    "model_target = os.environ.get('model_target',\n",
    "                              \"model.xml\")  # model output file name\n",
    "data_dir = os.environ.get('data_dir',\n",
    "                          '../../data/')  # temporary directory for data\n",
    "input_columns = os.environ.get('input_columns',\n",
    "                               '[\"x\", \"y\", \"z\"]')  # input columns to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(master)\n",
    "if sys.version[0:3] == '3.6':\n",
    "    conf.set(\"spark.jars\", 'jpmml-sparkml-executable-1.5.12.jar')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(data_dir + data_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df = df.withColumn(\"x\", df.x.cast(DoubleType()))\n",
    "df = df.withColumn(\"y\", df.y.cast(DoubleType()))\n",
    "df = df.withColumn(\"z\", df.z.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=eval(input_columns),\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import org.apache.spark.ml.feature.RFormula\n",
    "\n",
    "# val irisData = spark.read.format(\"csv\").option(\"header\", \"true\").\n",
    "# option(\"inferSchema\", \"true\").load(\"Iris.csv\")\n",
    "# val irisSchema = irisData.schema\n",
    "\n",
    "# val rFormula = new RFormula().setFormula(\"Species ~ .\")\n",
    "# val dtClassifier = new DecisionTreeClassifier().setLabelCol(\n",
    "# rFormula.getLabelCol).setFeaturesCol(rFormula.getFeaturesCol)\n",
    "# val pipeline = new Pipeline().setStages(Array(rFormula, dtClassifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binEval = MulticlassClassificationEvaluator(). \\\n",
    "    setMetricName(\"accuracy\"). \\\n",
    "    setPredictionCol(\"prediction\"). \\\n",
    "    setLabelCol(\"label\")\n",
    "\n",
    "binEval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmmlBuilder = PMMLBuilder(sc, df_train, model)\n",
    "pmmlBuilder.buildFile(data_dir + model_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.042719,
   "end_time": "2021-01-28T16:00:26.871724",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/work/elyra-classification/train-trusted-ai.ipynb",
   "output_path": "/home/jovyan/work/elyra-classification/train-trusted-ai.ipynb",
   "parameters": {},
   "start_time": "2021-01-28T15:59:31.829005",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
